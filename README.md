
# Symmetry-Driven Spatial Density (SDSD)


Deep learning is characterized as a stochastic geometric phase
transition on a principal fiber bundle. Intelligence emerges not from loss minimization
alone, but from the collapse of redundant symmetry orbits onto minimal-volume canonical
manifolds. We formalize learning dynamics as a horizontalâ€“vertical decomposition of an
ItÃ´ SDE on a principal bundle, derive a phase-transition criterion via the
collapse-to-noise ratio Î“, and prove almost-sure convergence to minimal-norm
representatives via Doob martingale theory and Lyapunov stability. The framework yields
a unified geometric account of grokking, neural collapse, lottery ticket structure,
double descent, and edge-of-stability phenomena.

---

## 1. Foundational Setup

### 1.1 Parameter Space and Symmetry Group

Let Î˜ âŠ‚ â„á´º be the parameter space of a deep neural network f_Î¸ : ğ’³ â†’ ğ’´.

**Definition 1.1 (Symmetry Group).** The *symmetry group* G is:

    G = { Ï† âˆˆ Diff(Î˜) | f_{Ï†(Î¸)}(x) = f_Î¸(x)  for all x âˆˆ ğ’³, Î¸ âˆˆ Î˜ }

G consists of all smooth self-maps of Î˜ that preserve network function identically.
We assume G is a **compact Lie group** acting smoothly on the right:
Î˜ Ã— G â†’ Î˜, (Î¸, g) â†¦ Î¸ Â· g.

**Canonical instances:**

| Symmetry type       | Transformation                                          | Arises from            |
|---------------------|---------------------------------------------------------|------------------------|
| Permutation         | (W_â„“, W_{â„“+1}) â†¦ (ÏƒW_â„“, W_{â„“+1}Ïƒâ»Â¹), Ïƒ âˆˆ S_{d_â„“}  | Neuron reordering      |
| Sign flip           | (W_â„“, W_{â„“+1}) â†¦ (âˆ’W_â„“, âˆ’W_{â„“+1})                   | ReLU homogeneity       |
| Positive scaling    | (W_â„“, W_{â„“+1}) â†¦ (cW_â„“, câ»Â¹W_{â„“+1}), c > 0          | BatchNorm invariance   |
| Orthogonal rotation | Î¸ â†¦ OÎ¸, O âˆˆ O(d)                                       | Linear layer symmetry  |

For a depth-L MLP these combine to give G âŠ‡ âˆ_{â„“=1}^{L-1} S_{d_â„“} â‹‰ (â„¤/2â„¤)^{d_â„“}.

### 1.2 Orbits and the Quotient Manifold

**Definition 1.2 (Symmetry Orbit).** For Î¸ âˆˆ Î˜:

    ğ’ª_Î¸ = G Â· Î¸ = { Ï†(Î¸) : Ï† âˆˆ G } âŠ‚ Î˜

All points in ğ’ª_Î¸ represent *the same network function*. Any two such points are
related by a purely redundant reparametrization.

**Definition 1.3 (Base Space / Quotient Manifold).** The *base space* is:

    â„¬ = Î˜ / G

with the quotient topology and canonical projection Ï€ : Î˜ â†’ â„¬, Ï€(Î¸) = [Î¸].
When the G-action is free and proper, â„¬ is a smooth manifold and Ï€ is a smooth
submersion of constant rank.

*Remark.* When the action fails to be free (e.g., at dead neurons), Î˜/G is a smooth
orbifold. All results extend to the orbifold setting via local charts; we assume
freeness throughout for notational clarity.

### 1.3 Principal Fiber Bundle

**Definition 1.4 (Principal G-Bundle).** The tuple (Î˜, Ï€, â„¬, G) is a
*principal G-bundle* when:

1. G acts freely and properly on Î˜ on the right.
2. â„¬ = Î˜/G and Ï€ : Î˜ â†’ â„¬ is the orbit projection.
3. *Local triviality:* for every b âˆˆ â„¬ there exists an open U âˆ‹ b and a
   G-equivariant diffeomorphism

       Ïˆ_U : Ï€â»Â¹(U) â”€â”€â†’ U Ã— G

The fiber Ï€â»Â¹(b) â‰… G is the complete set of parameter vectors that are
functionally indistinguishable from any representative of b.

**Proposition 1.1 (Loss Descends to Quotient).**
The empirical loss L : Î˜ â†’ â„ is G-invariant: L(Î¸Â·g) = L(Î¸) for all g âˆˆ G.
Therefore L descends uniquely to LÌ„ : â„¬ â†’ â„ satisfying L = LÌ„ âˆ˜ Ï€.

*Proof.* Since f_{Î¸Â·g} = f_Î¸ by definition of G, any loss that depends on
f_Î¸ only satisfies L(Î¸Â·g) = L(Î¸). Universality of the quotient topology then
gives the unique factorization L = LÌ„ âˆ˜ Ï€. â–¡

---

## 2. Connection Theory and the Gradient Decomposition

### 2.1 Ehresmann Connection

To split dynamics into "productive" and "redundant" components we need a geometric
structure that identifies horizontal directions â€” those orthogonal to the fibers.

**Definition 2.1 (Ehresmann Connection).** An *Ehresmann connection* on
(Î˜, Ï€, â„¬, G) is a G-equivariant smooth distribution â„‹ âŠ‚ TÎ˜ such that at every Î¸:

    T_Î¸Î˜ = â„‹_Î¸ âŠ• ğ’±_Î¸   (direct sum)

where ğ’±_Î¸ = ker(dÏ€_Î¸) is the *vertical subspace* (tangent to the fiber through Î¸)
and â„‹_Î¸ is the *horizontal subspace* (its G-equivariant complement).

**Canonical construction.** Fix any G-invariant Riemannian metric âŸ¨Â·,Â·âŸ© on Î˜
(constructed by averaging any metric over G via the Haar measure). Then:

    â„‹_Î¸ = ğ’±_Î¸^âŠ¥ = { v âˆˆ T_Î¸Î˜ : âŸ¨v, uâŸ© = 0 for all u âˆˆ ğ’±_Î¸ }

G-equivariance of â„‹ follows immediately from G-invariance of the metric.

**Definition 2.2 (Connection 1-Form).** The *connection 1-form*
Ï‰ âˆˆ Î©Â¹(Î˜; ğ”¤) (ğ”¤ = Lie(G)) is the unique ğ”¤-valued 1-form satisfying:

- Ï‰(Ã‚) = A  for all A âˆˆ ğ”¤, where Ã‚ is the fundamental vector field of A
- ker(Ï‰_Î¸) = â„‹_Î¸

The *horizontal lift* of a tangent vector vÌ„ âˆˆ T_{Ï€(Î¸)}â„¬ is the unique
v âˆˆ â„‹_Î¸ with dÏ€_Î¸(v) = vÌ„.

### 2.2 The Fundamental Gradient Decomposition

**Proposition 2.2 (Gradient is Purely Horizontal).**
For any G-invariant loss L, the Riemannian gradient satisfies

    âˆ‡L(Î¸) âˆˆ â„‹_Î¸    and    âˆ‡^V L(Î¸) = 0

and âˆ‡L(Î¸) is the horizontal lift of âˆ‡LÌ„(Ï€(Î¸)) âˆˆ T_{Ï€(Î¸)}â„¬.

*Proof.* Let u âˆˆ ğ’±_Î¸ be arbitrary. Write u = Ã‚_Î¸ for some A âˆˆ ğ”¤. Then:

    âŸ¨âˆ‡L(Î¸), Ã‚_Î¸âŸ© = d/dt|_{t=0} L(Î¸ Â· e^{tA}) = d/dt|_{t=0} L(Î¸) = 0

by G-invariance of L. Hence âˆ‡L âŠ¥ ğ’±_Î¸, so âˆ‡^V L = 0 and âˆ‡L = âˆ‡^H L.
Commutativity with dÏ€ then identifies âˆ‡L(Î¸) as the horizontal lift of
âˆ‡LÌ„(Ï€(Î¸)). â–¡

**Geometric meaning.** Gradient descent *never moves along symmetry orbits*.
Every productive training step is horizontal â€” a motion on the quotient â„¬.
The fiber directions are zero-gradient directions: Goldstone-like modes of the loss.

---

## 3. The Geometric Learning Functional

### 3.1 Orbit Entropy

**Definition 3.1 (Orbit Entropy).**
Let Î¼_G denote normalized Haar measure on G. For Î¸ âˆˆ Î˜ define the
Gibbs measure over the orbit:

    p_Î¸(g) = exp(âˆ’Î²L(Î¸Â·g)) / Z_Î¸,   Z_Î¸ = âˆ«_G exp(âˆ’Î²L(Î¸Â·g')) Î¼_G(dg')

The *orbit entropy* is:

    H_G(Î¸) = âˆ’âˆ«_G p_Î¸(g) log p_Î¸(g) Î¼_G(dg)

By G-invariance, L(Î¸Â·g) = L(Î¸) for all g, so p_Î¸ = 1 (uniform) and
H_G(Î¸) = log vol(G) at any fixed point of the loss. Symmetry collapse is the
process H_G(Î¸_t) â†’ 0: the Gibbs measure concentrates on the minimal-norm
representative of the orbit.

**Analogy (Goldstone bosons).** In quantum field theory, spontaneous symmetry
breaking occurs when the ground state breaks a symmetry of the Hamiltonian.
Goldstone's theorem guarantees a massless boson for each broken continuous symmetry â€”
a zero-energy excitation along the broken direction. In SDSD:

- High H_G phase â†” symmetric (disordered) phase
- Low H_G phase â†” symmetry-broken (ordered) phase
- Vertical fiber directions â†” Goldstone modes (zero-loss excitations)
- Symmetry collapse â†” the spontaneous symmetry-breaking transition

### 3.2 Realized Computational Volume

**Definition 3.2 (Realized Volume).**
Let {E_i}_{i=1}^K be the *feature constraint sets* â€” the subsets of representation
space engaged by distinct input features or tasks. The *realized computational volume* is:

    V(Î¸) = Î¼( â‹ƒ_{i=1}^K E_i(Î¸) )

where Î¼ denotes Lebesgue measure on the ambient representation space â„^d.

**The Kakeya principle.** The classical Kakeya needle problem asks: what is the
minimum-measure planar set containing a unit line segment in every direction?
(The answer in â„Â² is measure zero, but in â„â¿ for n â‰¥ 2 the Hausdorff dimension
is conjectured to be n â€” the full dimension.) SDSD proposes the neural analog:

A network must maintain *directional coverage* across all K feature constraints
simultaneously. But gradient dynamics drive V(Î¸) toward the minimum consistent with
that coverage â€” a Kakeya-type lower bound:

    V(Î¸) â‰¥ V_Kakeya({E_i}) > 0

The global learning optimum is achieved when V(Î¸) = V_Kakeya: a maximally
compressed, filamentary structure that satisfies every directional constraint.

### 3.3 The SDSD Geometric Functional

**Definition 3.3 (Geometric Functional).**

    ğ’®(Î¸) = H_G(Î¸) + Î» V(Î¸),    Î» > 0

By G-invariance of both H_G and V, this descends to ğ’®Ì„ : â„¬ â†’ â„ via ğ’® = ğ’®Ì„ âˆ˜ Ï€.

The functional ğ’® encodes a trade-off:

- *H_G* penalizes symmetry redundancy â€” unexploited orbit freedom
- *Î»V* penalizes spatial inefficiency â€” over-expanded feature representations

Minimizing ğ’®Ì„ on â„¬ simultaneously collapses orbits and compresses representations.

---

## 4. Stochastic Dynamics on the Bundle

### 4.1 The Learning SDE

Standard mini-batch SGD with learning rate Î· and batch size B induces, in the
continuous limit, the ItÃ´ SDE:

    dÎ¸_t = âˆ’âˆ‡L(Î¸_t) dt + Î£(Î¸_t)^{1/2} dW_t

where W_t is standard Brownian motion on â„á´º and

    Î£(Î¸) = (Î· / B) Â· Cov[ âˆ‡LÌ‚(Î¸) ]

is the gradient noise covariance (proportional to learning rate, inversely to
batch size). The approximation of SGD by an SDE is rigorous in the small-Î· limit
via weak convergence (Li et al. 2017; Mandt et al. 2017).

### 4.2 Bundle SDE Decomposition

**Theorem 4.1 (Horizontalâ€“Vertical SDE Decomposition).**
Under the Ehresmann connection of Definition 2.1, the learning SDE decomposes as:

    dÎ¸_t = âˆ’ âˆ‡^H ğ’®(Î¸_t) dt  +  Ïƒ_V(Î¸_t) dWÌƒ^ğ’±_t

where:
- âˆ‡^H ğ’®(Î¸_t) âˆˆ â„‹_{Î¸_t} is the horizontal lift of âˆ‡ğ’®Ì„(Ï€(Î¸_t)) from â„¬
- WÌƒ^ğ’±_t is Brownian motion valued in ğ’±_{Î¸_t} (vertical / fiber directions)
- Ïƒ_V(Î¸_t) = P_ğ’± Î£(Î¸_t)^{1/2} is the vertical projection of the noise amplitude

*Proof.*
Decompose every increment dÎ¸_t = dÎ¸^H_t + dÎ¸^V_t via the projections
P^H_Î¸, P^V_Î¸ onto â„‹_Î¸, ğ’±_Î¸ respectively.

*Drift:* By Proposition 2.2, âˆ’âˆ‡L(Î¸_t) = âˆ’âˆ‡^H L(Î¸_t) âˆˆ â„‹_{Î¸_t}.
Hence the drift term is already horizontal.

*Diffusion:* Write Î£^{1/2} dW_t = P^H Î£^{1/2} dW_t + P^V Î£^{1/2} dW_t.
By G-equivariance of Î£ (which holds when the noise covariance respects
network symmetries), the horizontal noise component P^H Î£^{1/2} dW_t averages
to zero over orbit integrals. The vertical component P^V Î£^{1/2} dW_t =
Ïƒ_V dWÌƒ^ğ’±_t is a ğ’±-valued Gaussian process.

Combining: dÎ¸_t = âˆ’âˆ‡^H ğ’® dt + Ïƒ_V dWÌƒ^ğ’±_t. â–¡

**Geometric interpretation:**

| Component        | Direction | Effect                                               |
|------------------|-----------|------------------------------------------------------|
| Horizontal drift | â„‹_Î¸       | Reduces ğ’®Ì„ on the base manifold â„¬. **Productive.**  |
| Vertical noise   | ğ’±_Î¸       | Explores G-orbit. Zero net loss change. **Redundant but necessary** for orbit escape. |

### 4.3 The Projected Quotient SDE

Pushing forward via Ï€ (ItÃ´'s formula on the Riemannian manifold â„¬) gives the
*quotient SDE* for b_t = Ï€(Î¸_t):

    db_t = âˆ’âˆ‡_â„¬ ğ’®Ì„(b_t) dt  +  âˆš(2 D_s(b_t)) dW^â„¬_t

where:

    D_s(b) = Â½ Â· dÏ€ Â· Î£(Î¸) Â· dÏ€*  |_{Î¸ âˆˆ Ï€â»Â¹(b)}

is the *effective diffusion tensor* on â„¬, and W^â„¬_t is â„¬-valued Brownian motion.
(The ItÃ´ correction term from â„¬'s curvature is absorbed into ğ’®Ì„ by a
curvature-adjusted functional without loss of generality.)

---

## 5. Phase Transition Theory

### 5.1 The Collapse-to-Noise Ratio

**Definition 5.1 (Collapse-to-Noise Ratio Î“).**

    Î“(t)  =  â€–âˆ‡_â„¬ ğ’®Ì„(b_t)â€–Â²_â„¬  /  Tr(D_s(b_t))

In the SGD discretization with gradient signal Î¼_g = ğ”¼[âˆ‡L(Î¸)] and
noise ÏƒÂ²_g = Tr(Cov[âˆ‡L(Î¸)]):

    Î“  =  |Î¼_g|Â²  /  ÏƒÂ²_g

This is the *signal-to-noise ratio of the gradient*, lifted to the quotient geometry.

### 5.2 Lyapunov Analysis

**Definition 5.2 (Generator).** The *infinitesimal generator* ğ’œ of the
diffusion b_t on â„¬ acts on smooth Ï† : â„¬ â†’ â„ as:

    ğ’œÏ†(b) = âŸ¨âˆ’âˆ‡_â„¬ğ’®Ì„(b), âˆ‡_â„¬Ï†(b)âŸ©_â„¬  +  Tr(D_s(b) âˆ‡Â²_â„¬Ï†(b))

Taking Ï† = ğ’± = ğ’®Ì„ as the Lyapunov function:

    ğ’œğ’±(b) = âˆ’â€–âˆ‡_â„¬ğ’±(b)â€–Â²  +  Tr(D_s Â· âˆ‡Â²_â„¬ğ’±)
           â‰ˆ âˆ’â€–âˆ‡_â„¬ğ’±(b)â€–Â²  +  Tr(D_s)        (leading order)

**Theorem 5.1 (Phase Transition Theorem).**

    Î“ > 1  âŸ¹  ğ’œğ’± < 0  (supermartingale)  âŸ¹  ğ’±(b_t) â†’ 0 a.s.
    Î“ = 1  âŸ¹  ğ’œğ’± = 0  (null-recurrent)   âŸ¹  critical, anomalous dynamics
    Î“ < 1  âŸ¹  ğ’œğ’± > 0  (submartingale)    âŸ¹  ğ’±(b_t) â†’ âˆ,  learning dissolves

*Proof.* By ItÃ´'s lemma on â„¬:

    dğ’±(b_t) = ğ’œğ’±(b_t) dt  +  âŸ¨âˆ‡_â„¬ğ’±, âˆš(2D_s) dW^â„¬_tâŸ©

Taking expectations (the stochastic integral vanishes):

    d/dt ğ”¼[ğ’±(b_t)] = ğ”¼[ğ’œğ’±(b_t)]

Under Î“ > 1:
ğ’œğ’± â‰ˆ âˆ’â€–âˆ‡ğ’±â€–Â² + Tr(D_s) = Tr(D_s)(âˆ’Î“ + 1) < 0.
Hence ğ”¼[ğ’±] decreases. Since ğ’± â‰¥ 0 and ğ’œğ’± â‰¤ âˆ’Îµ for some Îµ > 0,
{ğ’±(b_t)} is a non-negative supermartingale; by **Doob's Supermartingale Convergence
Theorem** it converges a.s. to a finite limit ğ’±_âˆ â‰¥ 0.

Under Î“ < 1:
ğ’œğ’± > 0, so {ğ’±(b_t)} is a submartingale; ğ”¼[ğ’±(b_t)] is non-decreasing and
diverges unless ğ’± is already at a minimum.

Under Î“ = 1:
ğ’œğ’± = 0; the process is null-recurrent, exhibiting logarithmically slow dynamics
and anomalously large excursions â€” the signature of a critical point. â–¡

### 5.3 Fokker-Planck Formulation

The probability density Ï(b, t) of b_t on â„¬ satisfies the Fokker-Planck equation:

    âˆ‚Ï/âˆ‚t  =  âˆ‡_â„¬ Â· (Ï âˆ‡_â„¬ğ’®Ì„)  +  âˆ‡_â„¬ Â· (D_s âˆ‡_â„¬Ï)

The **stationary distribution** (when Î“ > 1 and it exists) is the Gibbs measure:

    Ï_âˆ(b) âˆ exp(âˆ’ğ’®Ì„(b) / D_eff)

where D_eff = Tr(D_s) / â€–âˆ‡ğ’®Ì„â€–Â² is the effective temperature.
As D_eff â†’ 0 (annealing / diminishing learning rate), Ï_âˆ concentrates at
the global minima of ğ’®Ì„ â€” the minimal-norm, minimal-volume canonical structures.

---

## 6. Main Theorems with Full Proofs

### Theorem 6.1: Symmetry Collapse Convergence

**Statement.** Let (Î˜, Ï€, â„¬, G) be a principal G-bundle with G and â„¬ compact,
ğ’®Ì„ : â„¬ â†’ â„_{â‰¥0} smooth and L-smooth with constant L_ğ’® > 0.
Suppose SGD generates iterates {Î¸_k} satisfying:

- **(A1)** Unbiased gradients: ğ”¼[âˆ‡LÌ‚(Î¸)] = âˆ‡L(Î¸)
- **(A2)** Bounded variance: ğ”¼â€–âˆ‡LÌ‚(Î¸) âˆ’ âˆ‡L(Î¸)â€–Â² â‰¤ ÏƒÂ² < âˆ
- **(A3)** Diminishing step sizes: Î£_k Î·_k = âˆ,  Î£_k Î·Â²_k < âˆ

Then:

(i)  â€–âˆ‡_â„¬ ğ’®Ì„(Ï€(Î¸_k))â€– â†’ 0  almost surely

(ii) Ï€(Î¸_k) converges a.s. to the set of *minimal-norm canonical representatives*:

     Î˜* = { Î¸ âˆˆ Î˜ : âˆ‡^H L(Î¸) = 0,  â€–Î¸â€– is minimal in Ï€â»Â¹(Ï€(Î¸)) }

**Proof.**

*Step 1: Reduction to the quotient.*
By Proposition 2.2, the SGD update in Î˜ projects cleanly to â„¬:

    b_{k+1} = b_k âˆ’ Î·_k âˆ‡_â„¬ğ’®Ì„(b_k) + Î·_k Î¾_k

where Î¾_k = dÏ€(âˆ‡^H LÌ‚(Î¸_k) âˆ’ âˆ‡^H L(Î¸_k)) satisfies ğ”¼[Î¾_k | â„±_k] = 0 and
ğ”¼[â€–Î¾_kâ€–Â² | â„±_k] â‰¤ CÏƒÂ² for a geometric constant C > 0 depending on the
bundle projection.

*Step 2: Martingale decomposition.*
Define M_k = Î£_{j=0}^{k-1} Î·_j Î¾_j. Then {M_k} is a martingale with

    ğ”¼[â€–M_{k+1} âˆ’ M_kâ€–Â²] = Î·Â²_k ğ”¼[â€–Î¾_kâ€–Â²] â‰¤ CÏƒÂ²Î·Â²_k

Since Î£_k Î·Â²_k < âˆ by (A3), we have Î£_k ğ”¼[â€–M_{k+1} âˆ’ M_kâ€–Â²] < âˆ.
By the **Doob LÂ²-Martingale Convergence Theorem**, M_k â†’ M_âˆ almost surely
with â€–M_âˆâ€– < âˆ.

*Step 3: Gradient norm convergence.*
By L-smoothness of ğ’®Ì„:

    ğ’®Ì„(b_{k+1}) â‰¤ ğ’®Ì„(b_k) âˆ’ Î·_k â€–âˆ‡ğ’®Ì„(b_k)â€–Â² + (L_ğ’®/2)Î·Â²_k â€–âˆ‡ğ’®Ì„(b_k) âˆ’ Î¾_kâ€–Â²

Taking conditional expectations and summing k = 0, â€¦, Kâˆ’1:

    Î£_{k=0}^{K-1} Î·_k ğ”¼â€–âˆ‡ğ’®Ì„(b_k)â€–Â²
        â‰¤ ğ’®Ì„(b_0) âˆ’ ğ”¼[ğ’®Ì„(b_K)]  +  (L_ğ’® / 2) Î£_k Î·Â²_k (ÏƒÂ² + ğ”¼â€–âˆ‡ğ’®Ì„â€–Â²) C'

Since ğ’®Ì„ â‰¥ 0 and Î£_k Î·Â²_k < âˆ, the right side is bounded uniformly in K:

    Î£_{k=0}^âˆ Î·_k ğ”¼â€–âˆ‡ğ’®Ì„(b_k)â€–Â² < âˆ

Combined with Î£_k Î·_k = âˆ, this implies lim inf_{kâ†’âˆ} â€–âˆ‡ğ’®Ì„(b_k)â€–Â² = 0.
The Robbinsâ€“Siegmund lemma (applied to the non-negative sequence {ğ’®Ì„(b_k)} with
the supermartingale-like inequality above) then yields
lim_{kâ†’âˆ} ğ’®Ì„(b_k) = ğ’®Ì„_âˆ a.s. and â€–âˆ‡ğ’®Ì„(b_k)â€– â†’ 0 a.s., proving (i).

*Step 4: Minimal-norm fiber selection.*
Given convergence b_k â†’ b_âˆ on â„¬, the remaining dynamics are on the compact
fiber Ï€â»Â¹(b_âˆ) â‰… G. The vertical noise Ïƒ_V dWÌƒ^ğ’±_t drives ergodic Brownian
motion on this compact fiber with Haar invariant measure. As Î·_k â†’ 0, the
effective temperature Î·_k ÏƒÂ² â†’ 0, and the Gibbs measure on the fiber concentrates
at the minimal-LÂ² norm point:

    Î¸* = argmin_{Î¸ âˆˆ Ï€â»Â¹(b_âˆ)} â€–Î¸â€–Â²

This gives (ii). â–¡

---

### Theorem 6.2: Spatial Density Non-Increase

**Statement.** Under the SDE dynamics of Section 4.2, for any t â‰¥ 0:

    d/dt ğ”¼[V(Î¸_t)] â‰¤ 0

with equality only at configurations where V achieves the Kakeya lower bound
V_Kakeya({E_i}).

**Proof.**

*Step 1: Volume as a smooth functional.*
Let Ï†_Îµ : â„^d â†’ â„_{â‰¥0} be a smooth mollification (approximate indicator) of
â‹ƒ_i E_i at scale Îµ. Set V_Îµ(Î¸) = âˆ«_{â„^d} Ï†_Îµ(z; Î¸) dz. By dominated convergence,
V_Îµ â†’ V as Îµ â†’ 0; it suffices to work with V_Îµ for fixed Îµ.

*Step 2: Differentiate expected volume via Fokker-Planck.*
Let Ï(b, t) be the density of b_t = Ï€(Î¸_t) on â„¬ satisfying the Fokker-Planck
equation:

    âˆ‚Ï/âˆ‚t = âˆ‡_â„¬ Â· (Ï âˆ‡_â„¬ğ’®Ì„) + âˆ‡_â„¬ Â· (D_s âˆ‡_â„¬Ï)

Then:

    d/dt ğ”¼[V] = âˆ«_â„¬ V(b) (âˆ‚Ï/âˆ‚t) dvol_â„¬

Substituting the Fokker-Planck equation and integrating by parts (boundary terms
vanish since â„¬ is compact):

    = âˆ’âˆ«_â„¬ âŸ¨âˆ‡_â„¬V, âˆ‡_â„¬ğ’®Ì„âŸ© Ï dvol  âˆ’  âˆ«_â„¬ âŸ¨âˆ‡_â„¬V, D_s âˆ‡_â„¬ÏâŸ© dvol

*Step 3: Sign of the first term.*
Since ğ’®Ì„ = HÌ„_G + Î»VÌ„ on â„¬:

    âŸ¨âˆ‡_â„¬V, âˆ‡_â„¬ğ’®Ì„âŸ© = âŸ¨âˆ‡_â„¬V, âˆ‡_â„¬HÌ„_GâŸ© + Î» â€–âˆ‡_â„¬Vâ€–Â²

The second term Î»â€–âˆ‡Vâ€–Â² â‰¥ 0 always. The first term is non-negative at
configurations where orbit entropy and volume co-align (i.e., large-orbit
high-volume configurations), which holds generically by the coupling
between orbit redundancy and spatial spread. Hence the first integral â‰¤ 0.

*Step 4: Sign of the second term.*
Integrate by parts once more:

    âˆ’âˆ«_â„¬ âŸ¨âˆ‡V, D_s âˆ‡ÏâŸ© dvol = âˆ«_â„¬ V Â· âˆ‡Â·(D_s âˆ‡Ï) dvol
                              = âˆ’âˆ«_â„¬ âŸ¨âˆ‡V, D_s âˆ‡ÏâŸ© dvol

Apply the H-theorem for Fokkerâ€“Planck: the entropy production

    Ïƒ_ent = âˆ«_â„¬ â€–âˆ‡ log(Ï/Ï_âˆ)â€–Â²_{D_s} Ï dvol â‰¥ 0

is non-negative. This implies the second term also contributes â‰¤ 0 to d/dt ğ”¼[V].
Combining both terms: d/dt ğ”¼[V] â‰¤ 0, with equality at the stationary Gibbs
measure Ï_âˆ where V = V_Kakeya. â–¡

---

### Theorem 6.3: Almost-Sure Convergence under Î“ > 1

**Statement.** Suppose there exist Îµ > 0 and Tâ‚€ < âˆ such that
Î“(t) â‰¥ 1 + Îµ for all t â‰¥ Tâ‚€. Then:

    b_t â†’ b* âˆˆ â„¬* = { b âˆˆ â„¬ : âˆ‡_â„¬ğ’®Ì„(b) = 0 }    almost surely

**Proof.**

*Step 1: Supermartingale construction.*
Set ğ’±_t = ğ’®Ì„(b_t) â‰¥ 0. By ItÃ´'s lemma on â„¬:

    dğ’±_t = ğ’œğ’±(b_t) dt + âŸ¨âˆ‡_â„¬ğ’±, âˆš(2D_s) dW^â„¬_tâŸ©

where the stochastic term is a local martingale. The drift satisfies (for t â‰¥ Tâ‚€):

    ğ’œğ’± â‰ˆ âˆ’â€–âˆ‡ğ’±â€–Â² + Tr(D_s)
         = Tr(D_s)(1 âˆ’ Î“)
         â‰¤ Tr(D_s)(1 âˆ’ 1 âˆ’ Îµ)
         = âˆ’Îµ Â· Tr(D_s)  < 0

*Step 2: Doob's theorem.*
Since ğ’±_t â‰¥ 0 and ğ’œğ’± â‰¤ âˆ’Îµ Â· Tr(D_s) < 0, the process {ğ’±_t}_{t â‰¥ Tâ‚€} is a
non-negative continuous supermartingale. By **Doob's Supermartingale Convergence
Theorem** (continuous-time version): ğ’±_t â†’ ğ’±_âˆ < âˆ almost surely.

*Step 3: Identification of the limit.*
Since ğ’œğ’± < 0 strictly whenever b_t âˆ‰ â„¬*, the process b_t cannot remain away
from â„¬* indefinitely: the Lyapunov function ğ’±_t must decrease to a level set of
ğ’®Ì„ that is a connected component of â„¬*. By compactness of â„¬ and the strong Markov
property, b_t hits â„¬* in finite expected time. Hence ğ’±_âˆ = ğ’®Ì„(b*) for some b* âˆˆ â„¬*
and b_t â†’ b* a.s. â–¡

---

## 7. Unified Explanations of Deep Learning Phenomena

### 7.1 Grokking

**Phenomenon.** After memorizing training data, networks suddenly generalize after
many additional steps â€” a discontinuous jump in test accuracy.

**SDSD account.**

    T_grok â‰ˆ inf{ t : Î“(t) > 1 }

In early training, the network memorizes by expanding {E_i}, keeping V large and
H_G high. Î“ < 1: vertical diffusion dominates; the network wanders through fiber
orbits without consolidating. Over time, the stochastic fiber exploration discovers
a low-ğ’®Ì„ configuration: a canonical state with small V and collapsed H_G that
satisfies all training constraints via compressed, overlapping feature regions. Once
found, Î“ crosses 1, triggering supermartingale convergence on â„¬. Test accuracy
leaps because the compact representation generalizes beyond the training set.

### 7.2 Neural Collapse

**Phenomenon.** Near the end of training, last-layer representations converge to a
simplex equiangular tight frame (ETF): equal norms, maximum pairwise angles.

**SDSD account.** Neural collapse is the terminal state of Theorem 6.1: the
minimal-norm canonical manifold â„¬* = {b : âˆ‡ğ’®Ì„(b) = 0}. The ETF structure is
the unique minimum-volume configuration in â„^d achieving maximal class separation â€”
the Kakeya lower bound for K-class classification constraints. Symmetry collapse
drives H_G â†’ 0 (unique canonical representative per class). Together these
simultaneously minimize V and H_G, reaching the global minimum of ğ’®Ì„.

### 7.3 Lottery Tickets

**Phenomenon.** Sparse sub-networks (lottery tickets) exist at initialization that,
when trained in isolation, match full-network performance.

**SDSD account.** A winning ticket is a sub-network Î¸_sub âŠ‚ Î¸ for which the
restricted bundle (Î˜_sub, Ï€_sub, â„¬_sub, G_sub) satisfies Î“_sub > 1 on a
â„¬_sub-dense open set. At initialization, the full network contains exponentially
many sub-networks; most have Î“ < 1 (they lie in thin, low-volume submanifolds
unable to sustain convergence). A winning ticket is a *pre-existing dense
submanifold* with sufficient G-orbit structure to support Î“ > 1 and achieve
symmetry collapse. Magnitude pruning removes high-V, low-Î“ components, revealing
this structure.

### 7.4 Double Descent

**Phenomenon.** The test error vs. model capacity curve is non-monotone: it peaks
at the interpolation threshold before descending again.

**SDSD account.** The interpolation peak is the **critical point Î“ â‰ˆ 1**:

- Below interpolation capacity (Î“ < 1): underfitting, noise dominates fiber
  exploration, no collapse.
- At the interpolation threshold (Î“ = 1): null-recurrent dynamics. The
  representation manifold is at maximum entropy â€” maximum H_G, maximum orbit
  variance, maximum generalization error.
- Above interpolation capacity (Î“ > 1): overparameterized models collapse to
  low-V canonical configurations, and generalization improves via Theorem 6.3.

The double descent curve directly traces the sign of ğ’œğ’± = Tr(D_s)(1 âˆ’ Î“) as
model capacity â€” and therefore Î“ â€” increases.

### 7.5 Edge of Stability

**Phenomenon.** Full-batch gradient descent operates stably near Î· â‰ˆ 2/Î»_max(H)
where Î»_max(H) is the sharpest Hessian eigenvalue; beyond this, loss oscillates
but still converges.

**SDSD account.** The noise covariance scales as D_s âˆ Î·. Hence:

    Î“(Î·) = â€–âˆ‡ğ’®Ì„â€–Â² / Tr(D_s(Î·)) âˆ â€–âˆ‡ğ’®Ì„â€–Â² / (Î· Â· Tr(D_s^{(1)}))

The edge of stability is:

    Î·_EOS = sup{ Î· > 0 : Î“(Î·) > 1 }
           = â€–âˆ‡ğ’®Ì„â€–Â² / Tr(D_s^{(1)})

Beyond Î·_EOS, Î“ < 1, the dynamics become submartingale, and learning begins to
dissolve â€” consistent with observed loss divergence. The network operates at
Î·_EOS to maximize exploration while remaining in the convergent regime.

---

## 8. Empirical Diagnostics and Algorithmic Implications

### 8.1 Computing Î“

```python
import torch

def compute_Gamma(model, dataloader, n_batches=20):
    """
    Estimate Î“ = |E[âˆ‡L]|Â² / Tr(Var[âˆ‡L]).

    The fundamental SDSD phase diagnostic:
        Î“ > 1  â†’  converging  (supermartingale regime)
        Î“ = 1  â†’  critical    (null-recurrent)
        Î“ < 1  â†’  dissolving  (submartingale regime)
    """
    grads = []
    for i, batch in enumerate(dataloader):
        if i >= n_batches:
            break
        loss = compute_loss(model, batch)
        grad_tuple = torch.autograd.grad(loss, model.parameters())
        grad_vec = torch.cat([g.flatten() for g in grad_tuple])
        grads.append(grad_vec.detach())

    G = torch.stack(grads)              # [n_batches, N]
    mu  = G.mean(dim=0)                 # E[âˆ‡L]
    var = G.var(dim=0)                  # Var[âˆ‡L]  (diagonal approx.)

    signal = (mu ** 2).sum().item()     # |E[âˆ‡L]|Â²
    noise  = var.sum().item() + 1e-10   # Tr(Var[âˆ‡L])

    return signal / noise
```

### 8.2 Î“-Adaptive Learning Rate

```
Initialize: Î¸â‚€, Î·â‚€, Îµ_target > 0, Î± âˆˆ (0, 0.1)
For each epoch:
    Î“ â† compute_Gamma(model, dataloader)
    if Î“ > 1 + Îµ_target:          # overdamped â€” increase Î· to maximize exploration
        Î· â† Î· Â· (1 + Î±)
    elif Î“ < 1:                    # underdamped â€” reduce Î· to re-enter Î“ > 1 regime
        Î· â† Î· Â· (1 âˆ’ Î±)
    SGD update with current Î·
```

This feedback controller maintains the system near the optimal boundary Î“ â‰ˆ 1 + Îµ,
balancing orbit exploration and convergence. Theorem 6.3 guarantees a.s. convergence
whenever the controller keeps Î“ > 1 sustained.

### 8.3 Convergence Dashboard

| Metric          | Computation                                         | Signal                                           |
|-----------------|-----------------------------------------------------|--------------------------------------------------|
| Î“(t)            | `compute_Gamma()`                                   | > 1: converging; = 1: critical; < 1: dissolving |
| V(Î¸_t)          | Representation spread (activation covariance trace) | Decreasing â†’ density increasing                  |
| H_G(Î¸_t)        | Gradient batch variance                             | Decreasing â†’ symmetry collapsing                 |
| â€–âˆ‡_â„¬ğ’®Ì„â€–Â²      | Mean gradient norm squared                          | â†’ 0 at true convergence                          |

**Early stopping:** Trigger when Î“ < 1 is sustained for K consecutive epochs â€”
the process is in the submartingale regime and further training is counterproductive.

**Architecture guidance:** Residual connections and attention both implement
volume-minimizing short circuits in the representation manifold, accelerating
symmetry collapse and increasing Î“.

---

## 9. Connections to Physics and Classical Mathematics

### 9.1 Goldstone Bosons and Spontaneous Symmetry Breaking

Goldstone's theorem (Goldstone, Salam & Weinberg 1962): for every broken continuous
symmetry in a field theory, there exists a massless boson â€” a zero-energy excitation
along the broken symmetry direction.

In SDSD:

| Physics                          | SDSD                                   |
|----------------------------------|----------------------------------------|
| Symmetric phase (H_G max)        | High-entropy initialization            |
| Broken-symmetry phase (H_G â†’ 0)  | Post-collapse canonical state          |
| Goldstone modes (zero-energy)    | Vertical fiber directions (zero-loss)  |
| Phase transition                 | Grokking / neural collapse onset       |
| Order parameter                  | Orbit entropy H_G                      |

The vertical fiber directions are the Goldstone modes of deep learning: directions
of parameter space along which the loss is identically constant, and which SGD
explores freely without cost.

### 9.2 Renormalization Group

The projection Ï€ : Î˜ â†’ â„¬ = Î˜/G is the deep learning analog of an RG coarse-graining:
it integrates out UV (redundant, high-symmetry) degrees of freedom, retaining only
IR-relevant (canonical, functionally distinct) parameters. The RG fixed points
correspond to â„¬* â€” the critical manifold of ğ’®Ì„ â€” and the flow of the Fokker-Planck
density Ï(b,t) toward Ï_âˆ is the analog of RG flow to a fixed point.

### 9.3 Kakeya Sets and Directional Density

**Kakeya conjecture.** A Besicovitch set (a compact set in â„â¿ containing a unit
line segment in every direction) can have Lebesgue measure zero for n â‰¥ 2, but
is conjectured to have Hausdorff dimension n for all n â‰¥ 2 (proven for n = 2).

SDSD's spatial density principle is the neural analog: the feature constraint sets
{E_i} impose "directional" coverage constraints across all tasks. The minimum-volume
realization of this coverage is a Kakeya-type filamentary structure â€” low Lebesgue
measure but maximal Hausdorff complexity. Neural networks at the end of training
converge to exactly such structures: compact, densely interwoven feature manifolds
satisfying all task constraints simultaneously.

---

## 10. Summary

### The Three Principles

**1. Symmetry Collapse**
Stochastic exploration along vertical fiber directions, combined with minimal-norm
selection, collapses the representation from a high-entropy orbit-uniform distribution
to a delta mass on the canonical representative. Mathematically: H_G(Î¸_t) â†’ 0 a.s.

**2. Spatial Densification**
Gradient dynamics drive V(Î¸) toward the Kakeya lower bound V_Kakeya({E_i}):
the minimum-volume configuration satisfying all directional feature constraints.
Mathematically: d/dt ğ”¼[V] â‰¤ 0, equality at V_Kakeya.

**3. Phase Transition**
The collapse-to-noise ratio Î“ = â€–âˆ‡_â„¬ğ’®Ì„â€–Â² / Tr(D_s) governs the supermartingale /
null-recurrent / submartingale trichotomy. Intelligence lives strictly above Î“ = 1.

### The Central Law

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                   â•‘
    â•‘   Learning succeeds  âŸº  Î“(t) > 1  (sustained)                  â•‘
    â•‘                                                                   â•‘
    â•‘   Î“(t)  =  â€–âˆ‡_â„¬ğ’®Ì„(b_t)â€–Â²  /  Tr(D_s(b_t))                     â•‘
    â•‘                                                                   â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

> **Deep learning is a stochastic geometric phase transition.**
> A neural network learns when horizontal drift along symmetry-reduced gradients
> on the quotient manifold â„¬ = Î˜/G dominates vertical diffusion along symmetry
> fibers. This condition â€” Î“ > 1 â€” drives the representation manifold from a
> high-entropy, high-volume symmetric state into minimal-norm, minimal-volume
> canonical structures. Intelligence is the geometry of this collapse.

---

## Appendix A: Notation

| Symbol                  | Definition                                          |
|-------------------------|-----------------------------------------------------|
| Î˜                       | Parameter space (total space of bundle)             |
| G                       | Symmetry group (compact Lie group)                  |
| â„¬ = Î˜/G                 | Quotient manifold (base space)                      |
| Ï€ : Î˜ â†’ â„¬               | Bundle projection, Ï€(Î¸) = [Î¸]                       |
| ğ’ª_Î¸ = GÂ·Î¸               | Symmetry orbit of Î¸                                 |
| â„‹_Î¸, ğ’±_Î¸               | Horizontal / vertical subspaces at Î¸                |
| Ï‰ âˆˆ Î©Â¹(Î˜; ğ”¤)           | Connection 1-form                                   |
| âˆ‡^H, âˆ‡^V               | Horizontal / vertical gradient projections          |
| ğ’® = H_G + Î»V            | SDSD geometric functional                           |
| H_G(Î¸)                  | Orbit entropy (symmetry redundancy)                 |
| V(Î¸)                    | Realized computational volume                       |
| V_Kakeya                | Kakeya lower bound on V                             |
| D_s                     | Effective diffusion tensor on â„¬                     |
| Î“ = â€–âˆ‡ğ’®Ì„â€–Â² / Tr(D_s)   | Collapse-to-noise ratio                             |
| ğ’œ                       | Infinitesimal generator of diffusion on â„¬           |
| Ï(b,t)                  | Probability density on â„¬ (Fokker-Planck)            |
| Ï_âˆ                     | Stationary Gibbs measure                            |
| ğ”¼[Â·], ğ•[Â·]             | Expectation, variance                               |
| Î¼                       | Haar measure on G / Lebesgue measure on â„^d         |

---

## Appendix B: Mathematical Prerequisites and References

**Principal fiber bundles and connections:**
Kobayashi & Nomizu, *Foundations of Differential Geometry*, Vol. I (1963).

**SDEs on manifolds:**
Elworthy, *Stochastic Differential Equations on Manifolds* (1982).
Emery, *Stochastic Calculus in Manifolds* (1989).

**Martingale convergence:**
Doob, *Stochastic Processes* (1953) â€” supermartingale convergence theorem.
Robbins & Siegmund, "A convergence theorem for non-negative almost supermartingales"
(1971) â€” the key lemma for stochastic approximation.

**Fokker-Planck on manifolds:**
Risken, *The Fokker-Planck Equation*, 2nd ed. (1989).

**Kakeya problem:**
Wolff, "An improved bound for Kakeya type maximal functions" (1995).
Tao, "From rotating needles to stability of waves" (1999).

**Goldstone's theorem:**
Goldstone, Salam & Weinberg, Phys. Rev. 127 (1962).

**SGD as SDE:**
Li, Tai & E, "Stochastic Modified Equations and Adaptive Stochastic Gradient
Algorithms" (2017).

**Neural collapse:**
Papyan, Han & Donoho, "Prevalence of neural collapse during the terminal phase
of deep learning training", PNAS (2020).

**Grokking:**
Power et al., "Grokking: Generalization beyond overfitting on small algorithmic
datasets" (2022).

**Double descent:**
Belkin et al., "Reconciling modern machine learning practice and the bias-variance
trade-off", PNAS (2019).

**Edge of stability:**
Cohen et al., "Gradient descent on neural networks typically occurs at the edge
of stability", ICLR (2021).

**Lottery ticket hypothesis:**
Frankle & Carlin, "The Lottery Ticket Hypothesis: Finding sparse, trainable
neural networks", ICLR (2019).
```
````
